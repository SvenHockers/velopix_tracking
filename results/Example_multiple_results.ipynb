{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14bbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Any\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baff187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path:\n",
    "path = Path(\"results/\")\n",
    "reconstruction_algo = \"TF\"\n",
    "solver = \"GridSearch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_results(path: str) -> pd.DataFrame:\n",
    "    records = []\n",
    "    with open(path, 'r') as f:\n",
    "        buffer = []\n",
    "        depth = 0\n",
    "\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not line:\n",
    "                continue  # skip blank lines\n",
    "\n",
    "            # Track nesting of braces\n",
    "            depth += line.count('{')\n",
    "            depth -= line.count('}')\n",
    "\n",
    "            buffer.append(raw)\n",
    "            # If we’ve closed all opened braces, process one full JSON object\n",
    "            if depth == 0 and buffer:\n",
    "                chunk = ''.join(buffer)\n",
    "                obj = json.loads(chunk)\n",
    "                # same flattening logic as before\n",
    "                for uid, details in obj.items():\n",
    "                    params = details.pop('params')\n",
    "                    flat = {'id': uid, **details}\n",
    "                    for p, v in params.items():\n",
    "                        if isinstance(v, list) and len(v) == 2:\n",
    "                            flat[f'{p}_x'] = v[0]\n",
    "                            flat[f'{p}_y'] = v[1]\n",
    "                        else:\n",
    "                            flat[p] = v\n",
    "                    records.append(flat)\n",
    "                buffer = []\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce56bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(file_path: str):\n",
    "    stem = Path(file_path).stem            \n",
    "    config_name = stem.removeprefix(\"results_\")      \n",
    "    \n",
    "    config_path = Path(\"configurations\") / f\"{config_name}.json\"\n",
    "    \n",
    "    # 3) Open & load the JSON\n",
    "    with config_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        config = json.load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbea02ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def collect_best_records(\n",
    "    path,                # e.g. \"results/\"\n",
    "    solver,              # substring to match in filename\n",
    "    reconstruction_algo  # another substring to match\n",
    "):\n",
    "    path = Path(path)\n",
    "    group = {}\n",
    "\n",
    "    # 1) gather all matching files into `group[name] = df`\n",
    "    for file_path in path.iterdir():\n",
    "        if not file_path.is_file():\n",
    "            continue\n",
    "        name = file_path.name\n",
    "        if solver in name and reconstruction_algo in name:\n",
    "            # parse_results should return a DataFrame\n",
    "            group[name] = parse_results(file_path)\n",
    "\n",
    "    # 2) error if nothing matched\n",
    "    if not group:\n",
    "        raise Exception(\n",
    "            f\"No files found in {path!r} containing \"\n",
    "            f\"{solver!r} AND {reconstruction_algo!r}\"\n",
    "        )\n",
    "\n",
    "    # 3) build a config dict for each name\n",
    "    config = { name: get_config(name) for name in group }\n",
    "\n",
    "    # 4) for each df, pick the row(s) with max score and merge in config\n",
    "    best_slices = []\n",
    "    for name, df in group.items():\n",
    "        # select best‐score rows\n",
    "        best_df = df[df['score'] == df['score'].max()].copy()\n",
    "\n",
    "        # grab the config dict for this experiment\n",
    "        conf = config[name]\n",
    "        # turn it into a DataFrame with same index, then concat\n",
    "        conf_df = pd.DataFrame([conf] * len(best_df), index=best_df.index)\n",
    "\n",
    "        merged = pd.concat([best_df.reset_index(drop=True),\n",
    "                            conf_df.reset_index(drop=True)],\n",
    "                           axis=1)\n",
    "        best_slices.append(merged)\n",
    "\n",
    "    # 5) stitch all best‐records together\n",
    "    result = pd.concat(best_slices, ignore_index=True)\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "# df_best = collect_best_records(\"results/\", solver=\"GridSearch\", reconstruction_algo=\"TF\")\n",
    "# print(df_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d4f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best = collect_best_records(\"results/\", solver=\"GridSearch\", reconstruction_algo=\"TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1f612",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (3519850225.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfor df in group:\u001b[39m\n                    ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def plot_score_scatter_by_params(\n",
    "    df,\n",
    "    x_param,\n",
    "    y_param,\n",
    "    figsize=(8, 5),\n",
    "    cmap='viridis',\n",
    "    marker='o',\n",
    "    alpha=0.8,\n",
    "    title_fontsize=18,\n",
    "    label_fontsize=14,\n",
    "    tick_fontsize=12,\n",
    "    cbar_label='Score',\n",
    "    dpi=300,\n",
    "    title=\"INSERT A TITLE\",\n",
    "    XTitle=\"INSERT\",\n",
    "    YTitle=\"INSERT\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Scatter plot of score as color vs two hyperparameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing 'score' and hyperparameter columns.\n",
    "    x_param : str\n",
    "        Column name for x-axis.\n",
    "    y_param : str\n",
    "        Column name for y-axis.\n",
    "    figsize : tuple, default=(8, 5)\n",
    "        Figure size in inches.\n",
    "    cmap : str or Colormap, default='viridis'\n",
    "        Colormap for score values.\n",
    "    marker : str, default='o'\n",
    "        Marker style.\n",
    "    alpha : float, default=0.8\n",
    "        Alpha blending for markers.\n",
    "    title_fontsize, label_fontsize, tick_fontsize, dpi : int\n",
    "        Matplotlib styling parameters.\n",
    "    cbar_label : str, default='Score'\n",
    "        Label for the colorbar.\n",
    "    \"\"\"\n",
    "    # Configure rcParams for publication-quality styling\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\":       \"serif\",\n",
    "        \"font.serif\":        [\"Times New Roman\", \"Palatino\", \"serif\"],\n",
    "        \"axes.titlesize\":    title_fontsize,\n",
    "        \"axes.labelsize\":    label_fontsize,\n",
    "        \"xtick.labelsize\":   tick_fontsize,\n",
    "        \"ytick.labelsize\":   tick_fontsize,\n",
    "        \"figure.dpi\":        dpi,\n",
    "        \"axes.grid\":         True,\n",
    "        \"grid.color\":        \"#999999\",\n",
    "        \"grid.alpha\":        0.2,\n",
    "        \"grid.linestyle\":    \"--\"\n",
    "    })\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sc = ax.scatter(\n",
    "        df[x_param],\n",
    "        df[y_param],\n",
    "        # c=df['score'],\n",
    "        # cmap=cmap,\n",
    "        # marker=marker,\n",
    "        # alpha=alpha\n",
    "    )\n",
    "    ax.set_title(title, pad=10)\n",
    "    ax.set_xlabel(XTitle)\n",
    "    ax.set_ylabel(YTitle)\n",
    "\n",
    "    # Add colorbar\n",
    "    # cbar = plt.colorbar(sc, ax=ax)\n",
    "    # cbar.set_label(cbar_label)\n",
    "\n",
    "    # Rotate tick labels if needed\n",
    "    if df[x_param].nunique() > 10:\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    if df[y_param].nunique() > 10:\n",
    "        plt.setp(ax.get_yticklabels(), rotation=45, va='top')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d1a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_scatter_by_params(\n",
    "    df_best,\n",
    "    x_param='event_avg_ghost_rate',\n",
    "    y_param='clone_percentage',\n",
    "    figsize=(10, 6),\n",
    "    cmap='plasma',\n",
    "    marker='s',\n",
    "    alpha=0.7,\n",
    "    cbar_label='Mean Score',\n",
    "    title=\"Score by Slope\",\n",
    "    XTitle=\"Ghost Rate [%]\",\n",
    "    YTitle=\"Clone Rate [%]\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

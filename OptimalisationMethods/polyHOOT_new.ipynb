{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c6db58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import json\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from velopix_wrappers.optimizers import BaseOptimizer\n",
    "from velopix_wrappers.velopix_pipeline import TrackFollowingPipeline, GraphDFSPipeline, SearchByTripletTriePipeline\n",
    "from typing import Any, Dict\n",
    "import numpy as np\n",
    "\n",
    "from typing import Any, Dict, Literal, List\n",
    "import random\n",
    "import numpy as np\n",
    "from velopix_wrappers.optimizers import BaseOptimizer, pMap\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dbdfc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, bounds, parent: Any = None):\n",
    "        self.sum_reward = 0\n",
    "        self.visited = 0\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.bounds = bounds\n",
    "\n",
    "    def add_child(self, child: Any):\n",
    "        self.children.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6875c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyHoot(BaseOptimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_iterations: int = 100,\n",
    "        objective: Literal[\"min\", \"max\"] = \"min\"\n",
    "    ):\n",
    "        super().__init__(objective=objective, auto_eval={\"autoEval\": True, \"nested\": True, \"weights\": [1.0, 1.0, 1.0, -10.0]})\n",
    "        self.max_iterations = max_iterations\n",
    "        self.current_iteration = 0\n",
    "\n",
    "\n",
    "    def init(self) -> pMap:\n",
    "        \"\"\"\n",
    "        Initializes with a random point within bounds.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.cfg = self._algorithm.get_config()\n",
    "        self.bounds = self._algorithm.get_bounds()\n",
    "\n",
    "        self.alfa = 5\n",
    "        self.epsilon = 20\n",
    "        self.eta = 0.5\n",
    "\n",
    "        self.root = Node(bounds=self.bounds)  # Root node with no bounds\n",
    "        self.nodes = [self.root]\n",
    "        self.current_node = self.root\n",
    "\n",
    "        self.param_num = 0\n",
    "\n",
    "        for key, (typ, _) in self.cfg.items():\n",
    "            if typ is not bool:\n",
    "                self.param_num += 1\n",
    "\n",
    "        self.nu = 4 * self.param_num\n",
    "        self.ro = 1 / (4 * self.param_num)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        self.current_iteration += 1\n",
    "\n",
    "        #pregenerate trees of bounds\n",
    "        for key, (typ, _) in self.cfg.items():\n",
    "            if typ is bool:\n",
    "                current_leaves = [node for node in self.nodes if len(node.children) == 0]\n",
    "                for node in current_leaves: #NOTE: I think this is an infite loop no since we keep adding nodes to the nodes list in the loop we keep adding children forever no?\n",
    "                    #also changed logic since we were overriding the original bounds. (unless you changed it already then no, since current leaves is a copy of the current nodes)\n",
    "                    bounds_false = deepcopy(node.bounds)\n",
    "                    bounds_false[key] = False #TODO should this be an int or a true bool to ask group.. (changing  it to bool)\n",
    "                    node1 = Node(bounds=bounds_false, parent=node)\n",
    "                    \n",
    "                    bounds_true = deepcopy(node.bounds)\n",
    "                    bounds_true[key] = True #TODO should this be an int or a true bool to ask group..\n",
    "                    node2 = Node(bounds=bounds_true, parent=node)\n",
    "                    \n",
    "                    #I think we also want to add these as children to the parent node yeah? or in this case the current node (yes indeed)\n",
    "                    node.children.append(node1)\n",
    "                    node.children.append(node2)\n",
    "                    \n",
    "                    #leave unchanged.. (I don't think we ever use the node list again, but just in case)\n",
    "                    self.nodes.append(node1)\n",
    "                    self.nodes.append(node2)\n",
    "\n",
    "\n",
    "        node = self.root\n",
    "        depth = 0\n",
    "\n",
    "        while len(node.children):\n",
    "            node.visited += 1\n",
    "            node = node.children[0]\n",
    "            depth += 1\n",
    "\n",
    "\n",
    "        node.visited += 1\n",
    "\n",
    "        new_bounds = self.returnBounds(node.bounds)\n",
    "\n",
    "        node.add_child(Node(bounds=new_bounds[0], parent=node))\n",
    "        node.add_child(Node(bounds=new_bounds[1], parent=node))\n",
    "\n",
    "\n",
    "        pmap = self.returnPmap(node.bounds)\n",
    "\n",
    "        return pmap\n",
    "    \n",
    "\n",
    "\n",
    "    def next(self) -> pMap:\n",
    "        \"\"\"\n",
    "        Evaluates the current configuration and returns a new one.\n",
    "        \"\"\"\n",
    "        self.current_iteration += 1\n",
    "\n",
    "\n",
    "        # score = self.objective_func([1.0, 1.0, 1.0, -10.0])\n",
    "\n",
    "        # Evaluate the current configuration (from previous init/next call)\n",
    "        #score = self.objective_func([1.0, 1.0, 1.0, -10.0])\n",
    "        #so we already increased the count of the nodes but we still want to backprop the scors which should happen after a run so at the start of \n",
    "        # next we get the last score in history and trace back up the stack accordingly or just use the score above but faster to get the score from history??? TODO ask team\n",
    "        #TODO:backprop here FIXED?!?!\n",
    "        if hasattr(self, 'score_history') and len(self.score_history) > 0:\n",
    "            score = self.score_history[-1]\n",
    "            current = self.current_node\n",
    "            while current is not None:\n",
    "                current.sum_reward += score\n",
    "                current = current.parent\n",
    "        \n",
    "        node = self.root\n",
    "        depth = 0\n",
    "\n",
    "        while len(node.children):\n",
    "            node.visited += 1\n",
    "            if node.children[0].visited == 0:\n",
    "                node = node.children[0]\n",
    "            elif node.children[1].visited == 0:\n",
    "                node = node.children[1]\n",
    "            else:\n",
    "                node1_score = (node.children[0].sum_reward / node.children[0].visited) + (self.current_iteration ** (self.alfa/self.epsilon)) * (node.children[0].visited ** (self.eta - 1)) + (self.nu * (self.ro ** depth))\n",
    "                node2_score = (node.children[1].sum_reward / node.children[1].visited) + (self.current_iteration ** (self.alfa/self.epsilon)) * (node.children[1].visited ** (self.eta - 1)) + (self.nu * (self.ro ** depth))\n",
    "\n",
    "                if node1_score > node2_score:\n",
    "                    node = node.children[0]\n",
    "                else:\n",
    "                    node = node.children[1]\n",
    "\n",
    "            depth += 1\n",
    "\n",
    "        node.visited += 1\n",
    "\n",
    "        new_bounds = self.returnBounds(node.bounds)\n",
    "\n",
    "        node.add_child(Node(bounds=new_bounds[0], parent=node))\n",
    "        node.add_child(Node(bounds=new_bounds[1], parent=node))\n",
    "\n",
    "\n",
    "        pmap = self.returnPmap(node.bounds)\n",
    "        #keep track of the leaf node we expanded and rolled out for the backprop we we do next again.\n",
    "        self.current_node = node\n",
    "\n",
    "            \n",
    "        return pmap\n",
    "    \n",
    "\n",
    "    def is_finished(self) -> bool:\n",
    "        \"\"\"Determines if optimization is complete.\"\"\"\n",
    "        #TODO: possibly add a check for reaching target score\n",
    "        #TODO check with team but I think here we also want to perform backprop since if were finished we wont performn the last next we need to backprop so we do it here instead (maybe, depends when this function is called by the pipeline(TODO: check it), either way, if we don't it would just mean we skip the last iteration, which should not be that big of a problem)\n",
    "        finished = self.current_iteration >= self.max_iterations\n",
    "        if finished:\n",
    "            if hasattr(self, 'score_history') and len(self.score_history) > 0:\n",
    "                score = self.score_history[-1]\n",
    "                current = self.current_node\n",
    "                while current is not None:\n",
    "                    current.sum_reward += score\n",
    "                    current = current.parent\n",
    "                \n",
    "        return finished\n",
    "    \n",
    "\n",
    "    #return pmap based on bounds (split in middle) (rollout phase)\n",
    "    def returnPmap (self, bounds: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        new_pmap = {}\n",
    "        \n",
    "        for key, (typ, _) in self.cfg.items():\n",
    "            if typ is float:\n",
    "                new_pmap[key] = (bounds[key][0] + bounds[key][1]) / 2\n",
    "            elif typ is int:\n",
    "                new_pmap[key] = (int)((bounds[key][0] + bounds[key][1]) / 2)\n",
    "            elif typ is bool:\n",
    "                new_pmap[key] = bounds[key]\n",
    "            \n",
    "        return new_pmap\n",
    "\n",
    "\n",
    "    def returnBounds(self, bounds: Dict[str, Any]) -> List [Dict[str, Any]]: #TODO we currently split each of the bounds in half but we only want to split the axis with the \n",
    "                                                                                            # currently largest diameter (yes, should be done)\n",
    "\n",
    "                                                                                            # also its late my brain is cooked but should we skip over the keys that \n",
    "                                                                                            # are boolean as we currently try to split these? (I added that now, just in case)\n",
    "\n",
    "                                                                                            # we also never noremalize anywhere. (will do that after lunch, will be done impleicetly (how do you write that?) when calculating biggest bound)\n",
    "\n",
    "        map = self.returnPmap(bounds)\n",
    "        new_bounds = [deepcopy(bounds), deepcopy(bounds)]\n",
    "\n",
    "        \n",
    "        # find biggest bound\n",
    "        max_key = None\n",
    "        max_relative_diff = -1\n",
    "\n",
    "        for key, (typ, _) in self.cfg.items():\n",
    "            if typ is not bool:\n",
    "                original_range = self.bounds[key][1] - self.bounds[key][0]\n",
    "                current_range = bounds[key][1] - bounds[key][0]\n",
    "                relative_diff = current_range / original_range if original_range != 0 else 0\n",
    "\n",
    "                if relative_diff > max_relative_diff:\n",
    "                    max_relative_diff = relative_diff\n",
    "                    max_key = key\n",
    "\n",
    "        # Split only the key with the biggest relative size\n",
    "        if max_key is not None:\n",
    "            low, high = new_bounds[0][max_key]\n",
    "            new_bounds[0][max_key] = (low, map[max_key])\n",
    "\n",
    "            low, high = new_bounds[1][max_key]\n",
    "            new_bounds[1][max_key] = (map[max_key], high)\n",
    "\n",
    "            \n",
    "        return new_bounds\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2f1100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: velo_event_0.json\n",
      "Loading file: velo_event_1.json\n",
      "Loading file: velo_event_2.json\n",
      "Loading file: velo_event_3.json\n",
      "Loading file: velo_event_4.json\n",
      "Loading file: velo_event_5.json\n",
      "Loading file: velo_event_6.json\n",
      "Loading file: velo_event_7.json\n",
      "Loading file: velo_event_8.json\n",
      "Loading file: velo_event_9.json\n",
      "Loading file: velo_event_10.json\n",
      "Loading file: velo_event_11.json\n",
      "Loading file: velo_event_12.json\n",
      "Loading file: velo_event_13.json\n",
      "Loading file: velo_event_14.json\n",
      "Loading file: velo_event_15.json\n",
      "Loading file: velo_event_16.json\n",
      "Loading file: velo_event_17.json\n",
      "Loading file: velo_event_18.json\n",
      "Loading file: velo_event_19.json\n",
      "Loading file: velo_event_20.json\n",
      "Loading file: velo_event_21.json\n",
      "Loading file: velo_event_22.json\n",
      "Loading file: velo_event_23.json\n",
      "Loading file: velo_event_24.json\n",
      "Loading file: velo_event_25.json\n",
      "Loading file: velo_event_26.json\n",
      "Loading file: velo_event_27.json\n",
      "Loading file: velo_event_28.json\n",
      "Loading file: velo_event_29.json\n",
      "Loading file: velo_event_30.json\n",
      "Loading file: velo_event_31.json\n",
      "Loading file: velo_event_32.json\n",
      "Loading file: velo_event_33.json\n",
      "Loading file: velo_event_34.json\n",
      "Loading file: velo_event_35.json\n",
      "Loading file: velo_event_36.json\n",
      "Loading file: velo_event_37.json\n",
      "Loading file: velo_event_38.json\n",
      "Loading file: velo_event_39.json\n",
      "Loading file: velo_event_40.json\n",
      "Loading file: velo_event_41.json\n",
      "Loading file: velo_event_42.json\n",
      "Loading file: velo_event_43.json\n",
      "Loading file: velo_event_44.json\n",
      "Loading file: velo_event_45.json\n",
      "Loading file: velo_event_46.json\n",
      "Loading file: velo_event_47.json\n",
      "Loading file: velo_event_48.json\n",
      "Loading file: velo_event_49.json\n",
      "Loading file: velo_event_50.json\n",
      "Skipping problematic file: velo_event_51.json\n",
      "Loading file: velo_event_52.json\n",
      "Loading file: velo_event_53.json\n",
      "Loading file: velo_event_54.json\n",
      "Loading file: velo_event_55.json\n",
      "Loading file: velo_event_56.json\n",
      "Loading file: velo_event_57.json\n",
      "Loading file: velo_event_58.json\n",
      "Loading file: velo_event_59.json\n",
      "Loading file: velo_event_60.json\n",
      "Loading file: velo_event_61.json\n",
      "Loading file: velo_event_62.json\n",
      "Loading file: velo_event_63.json\n",
      "Loading file: velo_event_64.json\n",
      "Loading file: velo_event_65.json\n",
      "Loading file: velo_event_66.json\n",
      "Loading file: velo_event_67.json\n",
      "Loading file: velo_event_68.json\n",
      "Loading file: velo_event_69.json\n",
      "Loading file: velo_event_70.json\n",
      "Loading file: velo_event_71.json\n",
      "Loading file: velo_event_72.json\n",
      "Loading file: velo_event_73.json\n",
      "Loading file: velo_event_74.json\n",
      "Loading file: velo_event_75.json\n",
      "Loading file: velo_event_76.json\n",
      "Loading file: velo_event_77.json\n",
      "Loading file: velo_event_78.json\n",
      "Loading file: velo_event_79.json\n",
      "Loading file: velo_event_80.json\n",
      "Loading file: velo_event_81.json\n",
      "Loading file: velo_event_82.json\n",
      "Loading file: velo_event_83.json\n",
      "Loading file: velo_event_84.json\n",
      "Loading file: velo_event_85.json\n",
      "Loading file: velo_event_86.json\n",
      "Loading file: velo_event_87.json\n",
      "Loading file: velo_event_88.json\n",
      "Loading file: velo_event_89.json\n",
      "Loading file: velo_event_90.json\n",
      "Loading file: velo_event_91.json\n",
      "Loading file: velo_event_92.json\n",
      "Loading file: velo_event_93.json\n",
      "Loading file: velo_event_94.json\n",
      "Loading file: velo_event_95.json\n",
      "Loading file: velo_event_96.json\n",
      "Loading file: velo_event_97.json\n",
      "Loading file: velo_event_98.json\n",
      "Loading file: velo_event_99.json\n",
      "Loading file: velo_event_100.json\n",
      "Loading file: velo_event_101.json\n",
      "Loading file: velo_event_102.json\n",
      "Loading file: velo_event_103.json\n",
      "Loading file: velo_event_104.json\n",
      "Loading file: velo_event_105.json\n",
      "Loading file: velo_event_106.json\n",
      "Loading file: velo_event_107.json\n",
      "Loading file: velo_event_108.json\n",
      "Loading file: velo_event_109.json\n",
      "Loading file: velo_event_110.json\n",
      "Loading file: velo_event_111.json\n",
      "Loading file: velo_event_112.json\n",
      "Loading file: velo_event_113.json\n",
      "Loading file: velo_event_114.json\n",
      "Loading file: velo_event_115.json\n",
      "Loading file: velo_event_116.json\n",
      "Loading file: velo_event_117.json\n",
      "Loading file: velo_event_118.json\n",
      "Loading file: velo_event_119.json\n",
      "Loading file: velo_event_120.json\n",
      "Loading file: velo_event_121.json\n",
      "Loading file: velo_event_122.json\n",
      "Loading file: velo_event_123.json\n",
      "Loading file: velo_event_124.json\n",
      "Loading file: velo_event_125.json\n",
      "Loading file: velo_event_126.json\n",
      "Loading file: velo_event_127.json\n",
      "Loading file: velo_event_128.json\n",
      "Loading file: velo_event_129.json\n",
      "Loading file: velo_event_130.json\n",
      "Loading file: velo_event_131.json\n",
      "Loading file: velo_event_132.json\n",
      "Loading file: velo_event_133.json\n",
      "Loading file: velo_event_134.json\n",
      "Loading file: velo_event_135.json\n",
      "Loading file: velo_event_136.json\n",
      "Loading file: velo_event_137.json\n",
      "Loading file: velo_event_138.json\n",
      "Loading file: velo_event_139.json\n",
      "Loading file: velo_event_140.json\n",
      "Loading file: velo_event_141.json\n",
      "Loading file: velo_event_142.json\n",
      "Loading file: velo_event_143.json\n",
      "Loading file: velo_event_144.json\n",
      "Loading file: velo_event_145.json\n",
      "Loading file: velo_event_146.json\n",
      "Loading file: velo_event_147.json\n",
      "Loading file: velo_event_148.json\n",
      "Loading file: velo_event_149.json\n"
     ]
    }
   ],
   "source": [
    "events = []\n",
    "n_files = 150\n",
    "\n",
    "for i in range(0, n_files):\n",
    "    if i == 51:\n",
    "        \"\"\"\n",
    "        There's an issue with event 51 -> module_prefix_sum contains value 79 twice resulting in and indexing error when loading the event\n",
    "        \"\"\"\n",
    "        print(f\"Skipping problematic file: velo_event_{i}.json\")\n",
    "    else:    \n",
    "        print(f\"Loading file: velo_event_{i}.json\")\n",
    "        event_file = open(os.path.join(\"../DB/raw\", f\"velo_event_{i}.json\"))\n",
    "        json_data = json.loads(event_file.read())\n",
    "        events.append(json_data) # type: ignore\n",
    "        event_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ddd684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = TrackFollowingPipeline(events=events, intra_node=True) # type: ignore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4eded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimising:  13%|█▎        | 40/300 [01:30<07:35,  1.75s/it]"
     ]
    }
   ],
   "source": [
    "Optimiser = PolyHoot(max_iterations=243, objective=\"min\") # type: ignore\n",
    "optimal_parameters = pipeline.optimise_parameters(Optimiser, max_runs=300) # DO NOT remove max_runs, chances are that this will run forever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
